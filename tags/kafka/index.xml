<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka on mamonas.dev</title>
    <link>https://mamonas.dev/tags/kafka/</link>
    <description>Recent content in Kafka on mamonas.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Aug 2025 21:41:41 +0300</lastBuildDate><atom:link href="https://mamonas.dev/tags/kafka/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>kafka-replay-cli: A Lightweight Kafka Replay &amp; Debugging Tool</title>
      <link>https://mamonas.dev/posts/kafka-replay-cli/</link>
      <pubDate>Mon, 18 Aug 2025 21:41:41 +0300</pubDate>
      
      <guid>https://mamonas.dev/posts/kafka-replay-cli/</guid>
      <description>&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&#34;https://github.com/KonMam/kafka-replay-cli&#34;&gt;github.com/KonMam/kafka-replay-cli&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyPI&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/kafka-replay-cli/&#34;&gt;pypi.org/project/kafka-replay-cli&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-i-built-this&#34;&gt;Why I Built This&lt;/h2&gt;
&lt;p&gt;I wanted more hands-on &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Kafka&lt;/a&gt; experience - that&amp;rsquo;s the gist of it. Before this, I’d dealt with a few producers/consumers here and there, read the docs, and studied Kafka’s architectural design principles (very insightful read if you are interested in that sort of thing: &lt;a href=&#34;https://kafka.apache.org/documentation/&#34;&gt;https://kafka.apache.org/documentation/&lt;/a&gt;).
But there’s only so much you can learn with limited exposure and just reading, so I decided to spend some time tinkering and learning by doing.&lt;/p&gt;</description>
      <content>&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&#34;https://github.com/KonMam/kafka-replay-cli&#34;&gt;github.com/KonMam/kafka-replay-cli&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyPI&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/kafka-replay-cli/&#34;&gt;pypi.org/project/kafka-replay-cli&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-i-built-this&#34;&gt;Why I Built This&lt;/h2&gt;
&lt;p&gt;I wanted more hands-on &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Kafka&lt;/a&gt; experience - that&amp;rsquo;s the gist of it. Before this, I’d dealt with a few producers/consumers here and there, read the docs, and studied Kafka’s architectural design principles (very insightful read if you are interested in that sort of thing: &lt;a href=&#34;https://kafka.apache.org/documentation/&#34;&gt;https://kafka.apache.org/documentation/&lt;/a&gt;).
But there’s only so much you can learn with limited exposure and just reading, so I decided to spend some time tinkering and learning by doing.&lt;/p&gt;
&lt;h2 id=&#34;goals&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;There were a few things I wanted to achieve with this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get more Kafka experience - main goal.&lt;/li&gt;
&lt;li&gt;Integrate &lt;a href=&#34;https://duckdb.org/&#34;&gt;DuckDB&lt;/a&gt; - for the past year, I have seen a lot of hype around it and have started using it for some ad-hoc analysis. I enjoy using it, so I wanted to find a place for it.&lt;/li&gt;
&lt;li&gt;Have something to show at the end of it - meaning, find a real issue that people using Kafka might have and develop something around it, applying good practices.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;problem--mvp&#34;&gt;Problem &amp;amp; MVP&lt;/h2&gt;
&lt;p&gt;I needed to find a problem I could so-called &amp;lsquo;solve,&amp;rsquo; even if it had been done before. After some careful Googling and ChatGPT-ing, &lt;strong&gt;Kafka message replay&lt;/strong&gt; came up as something people either struggle with or need heavy tools to handle. The tool should be useful for someone who needs to reprocess events with filters or transformations, debugging, or migrating data between topics.&lt;/p&gt;
&lt;p&gt;The initial MVP I scoped was simple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic replay of messages with filters.&lt;/li&gt;
&lt;li&gt;Ability to dump Kafka topic data.&lt;/li&gt;
&lt;li&gt;Query dumped data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I wanted it lightweight, scriptable, and easy to use - no streaming engine, web UI, or over-engineering.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;The first decision I had to make was whether to use Python or Golang.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Arguments for Python&lt;/strong&gt; - I have the most experience with it and expected it would be easier and faster to develop.
&lt;strong&gt;Arguments for Golang&lt;/strong&gt; - In the long run, it would most likely be more performant. I would get more familiar with Golang.&lt;/p&gt;
&lt;p&gt;Due to my decision to have something tangible in a few days, I went with Python. Since it is a small tool and I didn’t know how much use it would get, I preferred not to worry about making it as performant as possible - premature optimization is the root of all evil, after all.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools used for this project:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kafka - the core thing I wanted to learn. Using the &lt;code&gt;confluent_kafka&lt;/code&gt; Python package, as it had all the features I needed.&lt;/li&gt;
&lt;li&gt;DuckDB - see above.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://typer.tiangolo.com/&#34;&gt;Typer&lt;/a&gt; - a library for building CLI applications. I had never used it before but liked the look and ergonomics it offered.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arrow.apache.org/docs/python/parquet.html&#34;&gt;PyArrow for Parquet&lt;/a&gt; - efficient storage; I’m used to working with it, and DuckDB can read from it. For alternatives could have used JSON or Avro, but JSON is inefficient for larger data volumes. Avro - might add support in the future.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dump Kafka topics into Parquet files&lt;/li&gt;
&lt;li&gt;Replay messages from Parquet back into Kafka&lt;/li&gt;
&lt;li&gt;Filter replays by timestamp range and key&lt;/li&gt;
&lt;li&gt;Optional throttling during replay&lt;/li&gt;
&lt;li&gt;Apply custom transform hooks to modify or skip messages&lt;/li&gt;
&lt;li&gt;Preview replays without sending messages using &lt;code&gt;--dry-run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Control output verbosity with &lt;code&gt;--verbose&lt;/code&gt; and &lt;code&gt;--quiet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Query message dumps with DuckDB SQL&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lessons-learned&#34;&gt;Lessons Learned&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Kafka&lt;/strong&gt; - Not as intimidating as expected and quite enjoyable. Both the official Kafka CLI tools and the Python integrations are mature.
&lt;strong&gt;DuckDB&lt;/strong&gt; - Currently limited use in the project, but good for what it does. I might add more use for it in the future or remove it to reduce bloat if it isn’t utilized.
&lt;strong&gt;Typer&lt;/strong&gt; - Enjoyed working with it a lot. Super easy to get a CLI tool going.
&lt;strong&gt;Testing&lt;/strong&gt; - Used &lt;code&gt;pytest&lt;/code&gt;. For unit tests, I didn’t want Kafka running for each test, so I used &lt;code&gt;MagicMock&lt;/code&gt; and &lt;code&gt;monkeypatch&lt;/code&gt; to simulate real objects - techniques I’ll keep in my pocket for future. For integration testing, I spun up a Docker container with a Kafka broker to test real usage of the CLI using &lt;code&gt;subprocess&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Main takeaway:&lt;/strong&gt;
It’s important to figure out your goals and think about the architecture before you start mashing on the keyboard. Deciding the project scope and dependencies early let me focus on the main features. It’s always a balancing act: what’s core, what’s nice to have, and how much time you want to spend.&lt;/p&gt;
&lt;h2 id=&#34;outcome--reflection&#34;&gt;Outcome &amp;amp; Reflection&lt;/h2&gt;
&lt;p&gt;Did I get more Kafka experience? Yes.
Does the tool do what I set out to make it do? Yes.
Is it the best thing since sliced bread? Highly unlikely.
Are there better tools for this use case? Probably.&lt;/p&gt;
&lt;p&gt;At the end of the day, this was a learning experience and I had fun. If someone uses it - great. If no one does - also great, it just means that I didn&amp;rsquo;t spend enough time researching real usage problems.&lt;/p&gt;
&lt;h2 id=&#34;installation--usage&#34;&gt;Installation &amp;amp; Usage&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install kafka-replay-cli
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kafka-replay-cli dump --help
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kafka-replay-cli replay --help
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;Thank you for reading.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Kafka Producers Explained: Partitioning, Batching, and Reliability</title>
      <link>https://mamonas.dev/posts/kafka-producers-explained/</link>
      <pubDate>Mon, 18 Aug 2025 21:39:05 +0300</pubDate>
      
      <guid>https://mamonas.dev/posts/kafka-producers-explained/</guid>
      <description>&lt;p&gt;A Kafka producer is the entry point for all data written to Kafka. It sends records to specific topic partitions, defines batching behavior, and controls how reliably data is delivered.&lt;/p&gt;
&lt;p&gt;This post covers the behaviors and configurations that influence the producer: partitioning, batching, delivery guarantees, and message structure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-does-a-kafka-producer-do&#34;&gt;What Does a Kafka Producer Do?&lt;/h2&gt;
&lt;p&gt;A Kafka producer is a client library integrated into applications to write messages to Kafka topics. When a message is sent, the producer determines:&lt;/p&gt;</description>
      <content>&lt;p&gt;A Kafka producer is the entry point for all data written to Kafka. It sends records to specific topic partitions, defines batching behavior, and controls how reliably data is delivered.&lt;/p&gt;
&lt;p&gt;This post covers the behaviors and configurations that influence the producer: partitioning, batching, delivery guarantees, and message structure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-does-a-kafka-producer-do&#34;&gt;What Does a Kafka Producer Do?&lt;/h2&gt;
&lt;p&gt;A Kafka producer is a client library integrated into applications to write messages to Kafka topics. When a message is sent, the producer determines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which partition the message should go to&lt;/li&gt;
&lt;li&gt;How to serialize the message for Kafka&lt;/li&gt;
&lt;li&gt;Whether to batch it with others&lt;/li&gt;
&lt;li&gt;How many acknowledgments are required before the message is considered delivered&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Producers are designed to balance speed, reliability, ordering, and throughput. Optimizing for one might require to compromise another.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;partitioning-strategies-routing-messages-to-partitions&#34;&gt;Partitioning Strategies: Routing Messages to Partitions&lt;/h2&gt;
&lt;p&gt;Kafka topics are split into partitions. Every message sent by a producer is written to one partition. This decision is made by a partitioner function.&lt;/p&gt;
&lt;h3 id=&#34;with-a-key&#34;&gt;With a Key&lt;/h3&gt;
&lt;p&gt;If a message has a key, Kafka hashes it using the Murmur2 algorithm and assigns the message to a partition using:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;partition = hash(key) % number_of_partitions
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This ensures all messages with the same key go to the same partition. Kafka guarantees message order within a partition, so key-based partitioning is how per-key ordering is maintained.&lt;/p&gt;
&lt;h3 id=&#34;without-a-key&#34;&gt;Without a Key&lt;/h3&gt;
&lt;p&gt;If the key is null, Kafka uses one of two strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Round-robin&lt;/strong&gt;: messages cycle through partitions in order. Used in older clients&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sticky partitioning&lt;/strong&gt;: the producer sends all messages to the same partition until the batch is sent, then picks a new one. Default in modern clients&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sticky partitioning improves batching efficiency while maintaining fair distribution over time.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;message-format-structure-and-serialization&#34;&gt;Message Format: Structure and Serialization&lt;/h2&gt;
&lt;p&gt;Kafka treats every message as a set of bytes. Each record includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Key (optional): used for partitioning. Serialized to bytes&lt;/li&gt;
&lt;li&gt;Value: the actual data payload. Serialized to bytes&lt;/li&gt;
&lt;li&gt;Headers (optional): metadata as key-value pairs&lt;/li&gt;
&lt;li&gt;Timestamp: assigned by the client or broker&lt;/li&gt;
&lt;li&gt;Partition + Offset: assigned by the broker after the message is stored&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kafka does not interpret or modify message content; it just stores and transmits byte arrays. Producers are responsible for serializing messages before sending them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example (Python):&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;producer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; KafkaProducer(value_serializer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; v: json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dumps(v)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;encode(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Efficient serialization improves throughput and reduces broker load. Avoid inefficient formats like uncompressed JSON unless specifically required by system constraints.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;batching-and-compression-optimizing-throughput&#34;&gt;Batching and Compression: Optimizing Throughput&lt;/h2&gt;
&lt;p&gt;Sending one message per request is inefficient. Kafka producers batch multiple records together per partition before sending them to the broker.&lt;/p&gt;
&lt;h3 id=&#34;key-configuration-options&#34;&gt;Key Configuration Options&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;batch.size&lt;/code&gt;: maximum size in bytes for a batch. Larger batches improve compression and throughput, but increase memory usage&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linger.ms&lt;/code&gt;: how long to wait before sending a batch, even if it is not full. Increases batching opportunities at the cost of latency&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compression.type&lt;/code&gt;: compresses full batches. Options include &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;lz4&lt;/code&gt;, &lt;code&gt;snappy&lt;/code&gt;, &lt;code&gt;zstd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;send()&lt;/code&gt; method is non-blocking. It queues the record in memory and returns immediately. The background sender thread flushes batches when &lt;code&gt;batch.size&lt;/code&gt; is reached or &lt;code&gt;linger.ms&lt;/code&gt; expires.&lt;/p&gt;
&lt;p&gt;Batching operates on a per-partition basis. As a result, applications that produce to a large number of partitions may experience reduced batching efficiency unless message flow is concentrated across fewer partitions.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;delivery-guarantees-configuring-reliability-and-ordering&#34;&gt;Delivery Guarantees: Configuring Reliability and Ordering&lt;/h2&gt;
&lt;p&gt;Kafka producers can trade reliability for speed using the &lt;code&gt;acks&lt;/code&gt; configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;acks=0&lt;/code&gt;: fire and forget. Fastest, but data may be lost&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acks=1&lt;/code&gt;: wait for leader. Reasonable balance for many use cases&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acks=all&lt;/code&gt;: wait for all in-sync replicas. Safest, with higher latency&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ordering-and-retries&#34;&gt;Ordering and Retries&lt;/h3&gt;
&lt;p&gt;Kafka guarantees ordering within a single partition. To maintain strict ordering, ensure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All related messages share the same key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max.in.flight.requests.per.connection &amp;lt;= 1&lt;/code&gt; when retries are enabled (to prevent out-of-order writes during retries)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;idempotence-and-transactions&#34;&gt;Idempotence and Transactions&lt;/h2&gt;
&lt;p&gt;By default, producers use at-least-once semantics, meaning retries may cause duplicate messages. Kafka provides stronger guarantees where needed.&lt;/p&gt;
&lt;h3 id=&#34;idempotent-producer&#34;&gt;Idempotent Producer&lt;/h3&gt;
&lt;p&gt;Enable with &lt;code&gt;enable.idempotence=true&lt;/code&gt;. This prevents duplicates during retries by assigning each producer a unique ID and tracking sequence numbers per partition.&lt;/p&gt;
&lt;p&gt;This guarantees exactly-once delivery per partition, assuming the producer does not crash and restart with a new ID.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Use this when:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Downstream systems cannot deduplicate&lt;/li&gt;
&lt;li&gt;Every message must be uniquely written (for example, financial systems)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Avoid using high &lt;code&gt;max.in.flight&lt;/code&gt; values with idempotence if ordering matters.&lt;/p&gt;
&lt;h3 id=&#34;transactional-producer&#34;&gt;Transactional Producer&lt;/h3&gt;
&lt;p&gt;Transactional producers enable atomic writes across multiple partitions or topics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requires:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A configured &lt;code&gt;transactional.id&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use of API methods: &lt;code&gt;begin_transaction()&lt;/code&gt;, &lt;code&gt;send()&lt;/code&gt;, &lt;code&gt;commit_transaction()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is critical for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exactly-once event processing pipelines&lt;/li&gt;
&lt;li&gt;Kafka Streams applications&lt;/li&gt;
&lt;li&gt;Coordinating multiple topic writes as a single atomic unit&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transactions ensure no duplicates, no partial writes, and consistent failure handling.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;A well-tuned Kafka producer is critical to balancing throughput, reliability, and resource efficiency. It&amp;rsquo;s important to understand your delivery requirements and system constraints before leaning into aggressive batching or strong guarantees as you trade higher throughput for it.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Kafka Consumers Explained: Pull, Offsets, and Parallelism</title>
      <link>https://mamonas.dev/posts/kafka-consumers-explained/</link>
      <pubDate>Mon, 18 Aug 2025 21:30:08 +0300</pubDate>
      
      <guid>https://mamonas.dev/posts/kafka-consumers-explained/</guid>
      <description>&lt;p&gt;Kafka is built for high throughput, scalability, and fault tolerance. At the core of this is its consumer model. Unlike traditional messaging systems, Kafka gives consumers full control over how they read data. This post explains how Kafka consumers work by focusing on three things: how they pull data, how offsets work, and how parallelism is achieved with consumer groups.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pulling-data-from-kafka&#34;&gt;Pulling Data from Kafka&lt;/h2&gt;
&lt;p&gt;Kafka producers push data to brokers. Consumers pull data from brokers. This setup is intentional. It gives consumers control over how fast they process data.&lt;/p&gt;</description>
      <content>&lt;p&gt;Kafka is built for high throughput, scalability, and fault tolerance. At the core of this is its consumer model. Unlike traditional messaging systems, Kafka gives consumers full control over how they read data. This post explains how Kafka consumers work by focusing on three things: how they pull data, how offsets work, and how parallelism is achieved with consumer groups.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pulling-data-from-kafka&#34;&gt;Pulling Data from Kafka&lt;/h2&gt;
&lt;p&gt;Kafka producers push data to brokers. Consumers pull data from brokers. This setup is intentional. It gives consumers control over how fast they process data.&lt;/p&gt;
&lt;p&gt;In push-based systems, if the producer is faster than the consumer, the consumer can get overwhelmed or crash. Kafka avoids this problem by letting consumers decide when to fetch data. This helps with backpressure and makes the system more reliable.&lt;/p&gt;
&lt;p&gt;Pulling also helps with batching. A consumer can fetch many messages in a single request. This reduces the number of network calls. In contrast, push systems must send each message one by one or hold back messages without knowing if the consumer is ready.&lt;/p&gt;
&lt;p&gt;One downside of pull-based systems is wasteful polling. A consumer might keep asking for data even if nothing is available. Kafka avoids this by letting the consumer wait until enough data is ready before responding. This keeps CPU usage low and throughput high.&lt;/p&gt;
&lt;p&gt;Kafka also avoids a model where brokers pull data from producers. That design would need every producer to store its own data. It would require more coordination and increase the risk of disk failure. Instead, Kafka stores data on the broker, where it can be managed and replicated more easily.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;how-kafka-offsets-work&#34;&gt;How Kafka Offsets Work&lt;/h2&gt;
&lt;p&gt;Kafka splits topics into partitions. Each message in a partition has a number called an offset. The offset marks the position of the message in the log.&lt;/p&gt;
&lt;p&gt;Offsets give consumers control. A consumer chooses where to start reading and can track what has already been processed. If a consumer crashes, it can pick up where it left off by using its last committed offset.&lt;/p&gt;
&lt;p&gt;Kafka does not track this progress for the consumer. The consumer is responsible for managing its own offsets. This is part of what makes Kafka scalable and efficient.&lt;/p&gt;
&lt;p&gt;By default, a consumer starts at offset zero. This means it will read all messages that are still available. It can also be configured to start at the latest offset to only read new data.&lt;/p&gt;
&lt;p&gt;Kafka only keeps data for a limited time. If a consumer tries to read from an offset that is too old, Kafka will return an error. In that case, the consumer must reset to the earliest or latest available offset.&lt;/p&gt;
&lt;h3 id=&#34;key-terms&#34;&gt;Key Terms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Offset&lt;/strong&gt;: The position of a message in a partition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Log End Offset&lt;/strong&gt;: The offset where the next message will be written.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Committed Offset&lt;/strong&gt;: The offset of the last message the consumer has finished processing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;delivery-options&#34;&gt;Delivery Options&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;At-most-once&lt;/strong&gt;: The consumer commits the offset before processing. If it crashes during processing, the message is lost.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At-least-once&lt;/strong&gt;: The consumer commits the offset after processing. If it crashes before committing, the message may be processed again.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exactly-once&lt;/strong&gt;: This uses Kafka transactions. The message and its offset are written together. If anything fails, nothing is committed. This guarantees no duplication and no loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;parallelism-with-consumer-groups&#34;&gt;Parallelism with Consumer Groups&lt;/h2&gt;
&lt;p&gt;Kafka uses consumer groups to scale out processing. A consumer group is a set of consumers working together to read from a topic.&lt;/p&gt;
&lt;p&gt;Kafka assigns each partition to only one consumer in the group. This avoids duplication and ensures order within each partition.&lt;/p&gt;
&lt;p&gt;When the group changes (for example, when consumers are added or removed), Kafka reassigns partitions to the available consumers.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;100 partitions and 100 consumers: each consumer handles one partition.&lt;/li&gt;
&lt;li&gt;100 partitions and 50 consumers: each consumer handles two partitions.&lt;/li&gt;
&lt;li&gt;50 partitions and 100 consumers: only 50 consumers do work, the rest are idle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kafka does not let multiple consumers read from the same partition in the same group. This would require the broker to manage shared state, which adds complexity. Instead, Kafka puts the responsibility on the consumer to track offsets. This makes the broker faster and simpler.&lt;/p&gt;
&lt;p&gt;The number of partitions controls how much you can scale out. More partitions allow for more parallelism. Choosing the right number of partitions is important for performance and resource usage.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Kafka gives consumers control over how they pull data, where they start, and how they scale. Pull-based reads avoid overload. Offsets make it easy to recover from failure. Consumer groups allow you to scale out processing.&lt;/p&gt;
&lt;p&gt;This design makes Kafka fast, reliable, and efficient at any scale.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;appendix-quick-reference&#34;&gt;Appendix: Quick Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Partition&lt;/strong&gt;: A subset of a topic. Used for parallel processing and message ordering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offset&lt;/strong&gt;: A number showing a message’s position in a partition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consumer Group&lt;/strong&gt;: A set of consumers that share the work of reading from a topic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rebalancing&lt;/strong&gt;: The process where Kafka reassigns partitions when consumers join or leave a group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delivery Types&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;At-most-once&lt;/em&gt;: Fast, but may lose messages.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;At-least-once&lt;/em&gt;: Reliable, but may duplicate messages.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Exactly-once&lt;/em&gt;: Most accurate, but needs Kafka transactions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>How Kafka Achieves High Throughput: A Breakdown of Its Log-Centric Architecture</title>
      <link>https://mamonas.dev/posts/how-kafka-achieves-high-throughput/</link>
      <pubDate>Mon, 18 Aug 2025 20:13:35 +0300</pubDate>
      
      <guid>https://mamonas.dev/posts/how-kafka-achieves-high-throughput/</guid>
      <description>&lt;p&gt;Kafka routinely handles millions of messages per second on commodity hardware. This performance isn&amp;rsquo;t accidental. It stems from deliberate architectural choices centered around log-based storage, OS-level optimizations, and minimal coordination between readers and writers.&lt;/p&gt;
&lt;p&gt;This post breaks down the core mechanisms that enable Kafka&amp;rsquo;s high-throughput design.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-append-only-log-storage&#34;&gt;1. Append-Only Log Storage&lt;/h2&gt;
&lt;p&gt;Each Kafka topic is split into partitions, and each partition is an append-only log. It is essentially a durable, ordered sequence of messages that are immutable once written.&lt;/p&gt;</description>
      <content>&lt;p&gt;Kafka routinely handles millions of messages per second on commodity hardware. This performance isn&amp;rsquo;t accidental. It stems from deliberate architectural choices centered around log-based storage, OS-level optimizations, and minimal coordination between readers and writers.&lt;/p&gt;
&lt;p&gt;This post breaks down the core mechanisms that enable Kafka&amp;rsquo;s high-throughput design.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-append-only-log-storage&#34;&gt;1. Append-Only Log Storage&lt;/h2&gt;
&lt;p&gt;Each Kafka topic is split into partitions, and each partition is an append-only log. It is essentially a durable, ordered sequence of messages that are immutable once written.&lt;/p&gt;
&lt;p&gt;To manage growing data size efficiently, Kafka breaks each partition’s log into multiple segment files. A segment is a file on disk that stores a continuous range of messages. New messages are always written to the active segment using low-level system calls like &lt;code&gt;write()&lt;/code&gt;. This write lands in the OS page cache, not written to disk immediately.&lt;/p&gt;
&lt;p&gt;Kafka delays calling &lt;code&gt;fsync()&lt;/code&gt; to flush data to disk, relying instead on configurable flush policies (based on time or size). This reduces disk I/O and improves performance, at the cost of brief durability gaps. Kafka mitigates this through replication across brokers.&lt;/p&gt;
&lt;p&gt;Over time, when a segment reaches a size threshold, it is closed and a new one is created. Older segments become read-only and are subject to log retention, compaction, or deletion based on topic settings.&lt;/p&gt;
&lt;p&gt;By aligning its write path with sequential disk I/O, Kafka avoids random seeks entirely. This makes reads and writes fast and predictable, even on spinning disks, and scales well with data volume.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-outperforming-traditional-queues-with-sequential-io&#34;&gt;2. Outperforming Traditional Queues with Sequential I/O&lt;/h2&gt;
&lt;p&gt;Traditional messaging systems often manage message delivery using per-consumer tracking and persistence mechanisms that can result in random disk access, especially during acknowledgment, redelivery, or crash recovery. While these systems are efficient in memory, random I/O patterns on disk introduce performance bottlenecks. For spinning disks, a single seek can take around 10 milliseconds, and disks can only perform one seek at a time.&lt;/p&gt;
&lt;p&gt;Kafka sidesteps this entirely by relying on sequential I/O. Writes are appended, and reads proceed in order. This design significantly improves disk efficiency, especially under load.&lt;/p&gt;
&lt;p&gt;By decoupling performance from data volume and enabling concurrent read/write access, Kafka makes efficient use of low-cost storage hardware, such as commodity SATA drives, without sacrificing performance.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-speeding-up-seeks-with-lightweight-indexing&#34;&gt;3. Speeding Up Seeks with Lightweight Indexing&lt;/h2&gt;
&lt;p&gt;Each segment file is accompanied by lightweight offset and timestamp indexes. These allow consumers to seek directly to specific message positions without scanning entire files, ensuring fast lookup even on large datasets.&lt;/p&gt;
&lt;p&gt;Since Kafka consumers track their own offsets and messages are immutable, there is no need to update shared state for acknowledgments or deletions. This eliminates coordination between readers and writers, reducing contention and enabling true parallelism.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-batching-to-maximize-io-efficiency&#34;&gt;4. Batching to Maximize I/O Efficiency&lt;/h2&gt;
&lt;p&gt;High-throughput systems must avoid the overhead of processing one message at a time. Kafka uses a message set abstraction to batch messages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Producers group messages before sending them.&lt;/li&gt;
&lt;li&gt;Brokers perform a single disk write per batch.&lt;/li&gt;
&lt;li&gt;Consumers fetch large batches with a single network call.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This batching reduces system calls, disk seeks, and protocol overhead. As a result, throughput improves significantly.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-zero-copy-data-transfer-with-sendfile&#34;&gt;5. Zero-Copy Data Transfer with &lt;code&gt;sendfile()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Conventional data transfer involves multiple memory copies:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Disk to kernel space (page cache)&lt;/li&gt;
&lt;li&gt;Kernel to user space (application buffer)&lt;/li&gt;
&lt;li&gt;User space back to kernel (socket buffer)&lt;/li&gt;
&lt;li&gt;Kernel to NIC buffer (for network)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Kafka avoids this overhead using the &lt;code&gt;sendfile()&lt;/code&gt; system call. This enables zero-copy data transfer from the page cache directly to the network stack, bypassing user space entirely.&lt;/p&gt;
&lt;p&gt;This reduces CPU usage and memory pressure, allowing near wire-speed data transfer even under heavy load.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;6-long-term-retention-without-performance-loss&#34;&gt;6. Long-Term Retention Without Performance Loss&lt;/h2&gt;
&lt;p&gt;Kafka’s append-only log model enables long-term message retention, even for days or weeks, without degrading performance. Because reads and writes are decoupled, and messages are not mutated post-write, old data remains accessible without impacting current workloads.&lt;/p&gt;
&lt;p&gt;This supports powerful use cases like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replaying messages for state recovery&lt;/li&gt;
&lt;li&gt;Late-arriving consumer processing&lt;/li&gt;
&lt;li&gt;Time-travel debugging and auditing&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Kafka’s high throughput is the result of system and architectural decisions that work together by design. Its log-centric model avoids random I/O, minimizes coordination, and takes full advantage of OS-level features like the page cache and zero-copy transfers.&lt;/p&gt;
&lt;p&gt;The result: Kafka handles massive data volumes not through abstract complexity, but by working with the OS instead of against it.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;appendix-key-terms&#34;&gt;Appendix: Key Terms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;write()&lt;/code&gt;: A system call that transfers data from user space to the OS page cache.&lt;/li&gt;
&lt;li&gt;Page cache: A memory buffer managed by the kernel.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fsync()&lt;/code&gt;: Forces data in the page cache to be flushed to disk.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sendfile()&lt;/code&gt;: A system call that sends data from a file directly to the network without copying to user space.&lt;/li&gt;
&lt;li&gt;Sequential I/O: Reading or writing data in a linear order. Much faster than random I/O, especially on HDDs.&lt;/li&gt;
&lt;li&gt;Random I/O: Accessing data at non-contiguous disk locations. This causes performance degradation due to disk seeks.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Further Reading:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/#design&#34;&gt;Kafka Official Design Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</content>
    </item>
    
  </channel>
</rss>
