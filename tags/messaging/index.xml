<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Messaging on mamonas.dev</title>
    <link>https://mamonas.dev/tags/messaging/</link>
    <description>Recent content in Messaging on mamonas.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Aug 2025 21:39:05 +0300</lastBuildDate><atom:link href="https://mamonas.dev/tags/messaging/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kafka Producers Explained: Partitioning, Batching, and Reliability</title>
      <link>https://mamonas.dev/posts/kafka-producers-explained/</link>
      <pubDate>Mon, 18 Aug 2025 21:39:05 +0300</pubDate>
      
      <guid>https://mamonas.dev/posts/kafka-producers-explained/</guid>
      <description>&lt;p&gt;A Kafka producer is the entry point for all data written to Kafka. It sends records to specific topic partitions, defines batching behavior, and controls how reliably data is delivered.&lt;/p&gt;
&lt;p&gt;This post covers the behaviors and configurations that influence the producer: partitioning, batching, delivery guarantees, and message structure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-does-a-kafka-producer-do&#34;&gt;What Does a Kafka Producer Do?&lt;/h2&gt;
&lt;p&gt;A Kafka producer is a client library integrated into applications to write messages to Kafka topics. When a message is sent, the producer determines:&lt;/p&gt;</description>
      <content>&lt;p&gt;A Kafka producer is the entry point for all data written to Kafka. It sends records to specific topic partitions, defines batching behavior, and controls how reliably data is delivered.&lt;/p&gt;
&lt;p&gt;This post covers the behaviors and configurations that influence the producer: partitioning, batching, delivery guarantees, and message structure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-does-a-kafka-producer-do&#34;&gt;What Does a Kafka Producer Do?&lt;/h2&gt;
&lt;p&gt;A Kafka producer is a client library integrated into applications to write messages to Kafka topics. When a message is sent, the producer determines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which partition the message should go to&lt;/li&gt;
&lt;li&gt;How to serialize the message for Kafka&lt;/li&gt;
&lt;li&gt;Whether to batch it with others&lt;/li&gt;
&lt;li&gt;How many acknowledgments are required before the message is considered delivered&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Producers are designed to balance speed, reliability, ordering, and throughput. Optimizing for one might require to compromise another.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;partitioning-strategies-routing-messages-to-partitions&#34;&gt;Partitioning Strategies: Routing Messages to Partitions&lt;/h2&gt;
&lt;p&gt;Kafka topics are split into partitions. Every message sent by a producer is written to one partition. This decision is made by a partitioner function.&lt;/p&gt;
&lt;h3 id=&#34;with-a-key&#34;&gt;With a Key&lt;/h3&gt;
&lt;p&gt;If a message has a key, Kafka hashes it using the Murmur2 algorithm and assigns the message to a partition using:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;partition = hash(key) % number_of_partitions
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This ensures all messages with the same key go to the same partition. Kafka guarantees message order within a partition, so key-based partitioning is how per-key ordering is maintained.&lt;/p&gt;
&lt;h3 id=&#34;without-a-key&#34;&gt;Without a Key&lt;/h3&gt;
&lt;p&gt;If the key is null, Kafka uses one of two strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Round-robin&lt;/strong&gt;: messages cycle through partitions in order. Used in older clients&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sticky partitioning&lt;/strong&gt;: the producer sends all messages to the same partition until the batch is sent, then picks a new one. Default in modern clients&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sticky partitioning improves batching efficiency while maintaining fair distribution over time.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;message-format-structure-and-serialization&#34;&gt;Message Format: Structure and Serialization&lt;/h2&gt;
&lt;p&gt;Kafka treats every message as a set of bytes. Each record includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Key (optional): used for partitioning. Serialized to bytes&lt;/li&gt;
&lt;li&gt;Value: the actual data payload. Serialized to bytes&lt;/li&gt;
&lt;li&gt;Headers (optional): metadata as key-value pairs&lt;/li&gt;
&lt;li&gt;Timestamp: assigned by the client or broker&lt;/li&gt;
&lt;li&gt;Partition + Offset: assigned by the broker after the message is stored&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kafka does not interpret or modify message content; it just stores and transmits byte arrays. Producers are responsible for serializing messages before sending them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example (Python):&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;producer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; KafkaProducer(value_serializer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; v: json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dumps(v)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;encode(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Efficient serialization improves throughput and reduces broker load. Avoid inefficient formats like uncompressed JSON unless specifically required by system constraints.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;batching-and-compression-optimizing-throughput&#34;&gt;Batching and Compression: Optimizing Throughput&lt;/h2&gt;
&lt;p&gt;Sending one message per request is inefficient. Kafka producers batch multiple records together per partition before sending them to the broker.&lt;/p&gt;
&lt;h3 id=&#34;key-configuration-options&#34;&gt;Key Configuration Options&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;batch.size&lt;/code&gt;: maximum size in bytes for a batch. Larger batches improve compression and throughput, but increase memory usage&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linger.ms&lt;/code&gt;: how long to wait before sending a batch, even if it is not full. Increases batching opportunities at the cost of latency&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compression.type&lt;/code&gt;: compresses full batches. Options include &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;lz4&lt;/code&gt;, &lt;code&gt;snappy&lt;/code&gt;, &lt;code&gt;zstd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;send()&lt;/code&gt; method is non-blocking. It queues the record in memory and returns immediately. The background sender thread flushes batches when &lt;code&gt;batch.size&lt;/code&gt; is reached or &lt;code&gt;linger.ms&lt;/code&gt; expires.&lt;/p&gt;
&lt;p&gt;Batching operates on a per-partition basis. As a result, applications that produce to a large number of partitions may experience reduced batching efficiency unless message flow is concentrated across fewer partitions.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;delivery-guarantees-configuring-reliability-and-ordering&#34;&gt;Delivery Guarantees: Configuring Reliability and Ordering&lt;/h2&gt;
&lt;p&gt;Kafka producers can trade reliability for speed using the &lt;code&gt;acks&lt;/code&gt; configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;acks=0&lt;/code&gt;: fire and forget. Fastest, but data may be lost&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acks=1&lt;/code&gt;: wait for leader. Reasonable balance for many use cases&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acks=all&lt;/code&gt;: wait for all in-sync replicas. Safest, with higher latency&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ordering-and-retries&#34;&gt;Ordering and Retries&lt;/h3&gt;
&lt;p&gt;Kafka guarantees ordering within a single partition. To maintain strict ordering, ensure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All related messages share the same key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max.in.flight.requests.per.connection &amp;lt;= 1&lt;/code&gt; when retries are enabled (to prevent out-of-order writes during retries)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;idempotence-and-transactions&#34;&gt;Idempotence and Transactions&lt;/h2&gt;
&lt;p&gt;By default, producers use at-least-once semantics, meaning retries may cause duplicate messages. Kafka provides stronger guarantees where needed.&lt;/p&gt;
&lt;h3 id=&#34;idempotent-producer&#34;&gt;Idempotent Producer&lt;/h3&gt;
&lt;p&gt;Enable with &lt;code&gt;enable.idempotence=true&lt;/code&gt;. This prevents duplicates during retries by assigning each producer a unique ID and tracking sequence numbers per partition.&lt;/p&gt;
&lt;p&gt;This guarantees exactly-once delivery per partition, assuming the producer does not crash and restart with a new ID.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Use this when:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Downstream systems cannot deduplicate&lt;/li&gt;
&lt;li&gt;Every message must be uniquely written (for example, financial systems)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Avoid using high &lt;code&gt;max.in.flight&lt;/code&gt; values with idempotence if ordering matters.&lt;/p&gt;
&lt;h3 id=&#34;transactional-producer&#34;&gt;Transactional Producer&lt;/h3&gt;
&lt;p&gt;Transactional producers enable atomic writes across multiple partitions or topics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requires:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A configured &lt;code&gt;transactional.id&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use of API methods: &lt;code&gt;begin_transaction()&lt;/code&gt;, &lt;code&gt;send()&lt;/code&gt;, &lt;code&gt;commit_transaction()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is critical for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exactly-once event processing pipelines&lt;/li&gt;
&lt;li&gt;Kafka Streams applications&lt;/li&gt;
&lt;li&gt;Coordinating multiple topic writes as a single atomic unit&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transactions ensure no duplicates, no partial writes, and consistent failure handling.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;A well-tuned Kafka producer is critical to balancing throughput, reliability, and resource efficiency. It&amp;rsquo;s important to understand your delivery requirements and system constraints before leaning into aggressive batching or strong guarantees as you trade higher throughput for it.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Kafka Consumers Explained: Pull, Offsets, and Parallelism</title>
      <link>https://mamonas.dev/posts/kafka-consumers-explained/</link>
      <pubDate>Mon, 18 Aug 2025 21:30:08 +0300</pubDate>
      
      <guid>https://mamonas.dev/posts/kafka-consumers-explained/</guid>
      <description>&lt;p&gt;Kafka is built for high throughput, scalability, and fault tolerance. At the core of this is its consumer model. Unlike traditional messaging systems, Kafka gives consumers full control over how they read data. This post explains how Kafka consumers work by focusing on three things: how they pull data, how offsets work, and how parallelism is achieved with consumer groups.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pulling-data-from-kafka&#34;&gt;Pulling Data from Kafka&lt;/h2&gt;
&lt;p&gt;Kafka producers push data to brokers. Consumers pull data from brokers. This setup is intentional. It gives consumers control over how fast they process data.&lt;/p&gt;</description>
      <content>&lt;p&gt;Kafka is built for high throughput, scalability, and fault tolerance. At the core of this is its consumer model. Unlike traditional messaging systems, Kafka gives consumers full control over how they read data. This post explains how Kafka consumers work by focusing on three things: how they pull data, how offsets work, and how parallelism is achieved with consumer groups.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pulling-data-from-kafka&#34;&gt;Pulling Data from Kafka&lt;/h2&gt;
&lt;p&gt;Kafka producers push data to brokers. Consumers pull data from brokers. This setup is intentional. It gives consumers control over how fast they process data.&lt;/p&gt;
&lt;p&gt;In push-based systems, if the producer is faster than the consumer, the consumer can get overwhelmed or crash. Kafka avoids this problem by letting consumers decide when to fetch data. This helps with backpressure and makes the system more reliable.&lt;/p&gt;
&lt;p&gt;Pulling also helps with batching. A consumer can fetch many messages in a single request. This reduces the number of network calls. In contrast, push systems must send each message one by one or hold back messages without knowing if the consumer is ready.&lt;/p&gt;
&lt;p&gt;One downside of pull-based systems is wasteful polling. A consumer might keep asking for data even if nothing is available. Kafka avoids this by letting the consumer wait until enough data is ready before responding. This keeps CPU usage low and throughput high.&lt;/p&gt;
&lt;p&gt;Kafka also avoids a model where brokers pull data from producers. That design would need every producer to store its own data. It would require more coordination and increase the risk of disk failure. Instead, Kafka stores data on the broker, where it can be managed and replicated more easily.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;how-kafka-offsets-work&#34;&gt;How Kafka Offsets Work&lt;/h2&gt;
&lt;p&gt;Kafka splits topics into partitions. Each message in a partition has a number called an offset. The offset marks the position of the message in the log.&lt;/p&gt;
&lt;p&gt;Offsets give consumers control. A consumer chooses where to start reading and can track what has already been processed. If a consumer crashes, it can pick up where it left off by using its last committed offset.&lt;/p&gt;
&lt;p&gt;Kafka does not track this progress for the consumer. The consumer is responsible for managing its own offsets. This is part of what makes Kafka scalable and efficient.&lt;/p&gt;
&lt;p&gt;By default, a consumer starts at offset zero. This means it will read all messages that are still available. It can also be configured to start at the latest offset to only read new data.&lt;/p&gt;
&lt;p&gt;Kafka only keeps data for a limited time. If a consumer tries to read from an offset that is too old, Kafka will return an error. In that case, the consumer must reset to the earliest or latest available offset.&lt;/p&gt;
&lt;h3 id=&#34;key-terms&#34;&gt;Key Terms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Offset&lt;/strong&gt;: The position of a message in a partition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Log End Offset&lt;/strong&gt;: The offset where the next message will be written.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Committed Offset&lt;/strong&gt;: The offset of the last message the consumer has finished processing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;delivery-options&#34;&gt;Delivery Options&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;At-most-once&lt;/strong&gt;: The consumer commits the offset before processing. If it crashes during processing, the message is lost.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At-least-once&lt;/strong&gt;: The consumer commits the offset after processing. If it crashes before committing, the message may be processed again.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exactly-once&lt;/strong&gt;: This uses Kafka transactions. The message and its offset are written together. If anything fails, nothing is committed. This guarantees no duplication and no loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;parallelism-with-consumer-groups&#34;&gt;Parallelism with Consumer Groups&lt;/h2&gt;
&lt;p&gt;Kafka uses consumer groups to scale out processing. A consumer group is a set of consumers working together to read from a topic.&lt;/p&gt;
&lt;p&gt;Kafka assigns each partition to only one consumer in the group. This avoids duplication and ensures order within each partition.&lt;/p&gt;
&lt;p&gt;When the group changes (for example, when consumers are added or removed), Kafka reassigns partitions to the available consumers.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;100 partitions and 100 consumers: each consumer handles one partition.&lt;/li&gt;
&lt;li&gt;100 partitions and 50 consumers: each consumer handles two partitions.&lt;/li&gt;
&lt;li&gt;50 partitions and 100 consumers: only 50 consumers do work, the rest are idle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kafka does not let multiple consumers read from the same partition in the same group. This would require the broker to manage shared state, which adds complexity. Instead, Kafka puts the responsibility on the consumer to track offsets. This makes the broker faster and simpler.&lt;/p&gt;
&lt;p&gt;The number of partitions controls how much you can scale out. More partitions allow for more parallelism. Choosing the right number of partitions is important for performance and resource usage.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Kafka gives consumers control over how they pull data, where they start, and how they scale. Pull-based reads avoid overload. Offsets make it easy to recover from failure. Consumer groups allow you to scale out processing.&lt;/p&gt;
&lt;p&gt;This design makes Kafka fast, reliable, and efficient at any scale.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;appendix-quick-reference&#34;&gt;Appendix: Quick Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Partition&lt;/strong&gt;: A subset of a topic. Used for parallel processing and message ordering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offset&lt;/strong&gt;: A number showing a messageâ€™s position in a partition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consumer Group&lt;/strong&gt;: A set of consumers that share the work of reading from a topic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rebalancing&lt;/strong&gt;: The process where Kafka reassigns partitions when consumers join or leave a group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delivery Types&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;At-most-once&lt;/em&gt;: Fast, but may lose messages.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;At-least-once&lt;/em&gt;: Reliable, but may duplicate messages.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Exactly-once&lt;/em&gt;: Most accurate, but needs Kafka transactions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
