<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data-Engineering on mamonas.dev</title>
    <link>http://localhost:1313/tags/data-engineering/</link>
    <description>Recent content in Data-Engineering on mamonas.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Aug 2025 22:04:52 +0300</lastBuildDate><atom:link href="http://localhost:1313/tags/data-engineering/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Which Compression Saves the Most Storage $? (gzip, Snappy, LZ4, zstd)</title>
      <link>http://localhost:1313/posts/which-compression-algorithm-saves-most/</link>
      <pubDate>Mon, 18 Aug 2025 22:04:52 +0300</pubDate>
      
      <guid>http://localhost:1313/posts/which-compression-algorithm-saves-most/</guid>
      <description>&lt;p&gt;Compression setting are set and forget in most cases, if it works no reason to change it. I decided to look into and see whether it would be beneficial to review the defaults and if it could save money. I covered most of the algorithms discussed in this post previously in &lt;a href=&#34;https://mamonas.dev/posts/compression-algorithms-you-probably-inherited/&#34;&gt;Compression Algorithms You Probably Inherited&lt;/a&gt;, where I summarized the info I collected while researching. But I wanted to sanity-check the findings myself and decided to run some benchmarks. This should help me see if compression actually makes a difference for storage costs.&lt;/p&gt;</description>
      <content>&lt;p&gt;Compression setting are set and forget in most cases, if it works no reason to change it. I decided to look into and see whether it would be beneficial to review the defaults and if it could save money. I covered most of the algorithms discussed in this post previously in &lt;a href=&#34;https://mamonas.dev/posts/compression-algorithms-you-probably-inherited/&#34;&gt;Compression Algorithms You Probably Inherited&lt;/a&gt;, where I summarized the info I collected while researching. But I wanted to sanity-check the findings myself and decided to run some benchmarks. This should help me see if compression actually makes a difference for storage costs.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-i-tested&#34;&gt;What I Tested&lt;/h2&gt;
&lt;p&gt;To keep things real, I used actual data: &lt;a href=&#34;https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page&#34;&gt;NYC TLC trip records&lt;/a&gt;. Each month’s data file was ~500MB. I combined a few to get files at 500MB, 1.6GB, 3.9GB, and 6.6GB.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Compression algorithms tested:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gzip&lt;/li&gt;
&lt;li&gt;Snappy&lt;/li&gt;
&lt;li&gt;LZ4&lt;/li&gt;
&lt;li&gt;zstd at levels 1, 3, 9, and 19&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Environment:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MacBook Pro (2021, M1 Pro, 16GB RAM)&lt;/li&gt;
&lt;li&gt;Single-threaded runs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I couldn’t process the largest file with my setup. SKILL ISSUE. In reality, I didn’t bother trying to fix it, multi-threading and batching the compression should have allowed me to do it, but I already had the 3 other files to work with.&lt;/p&gt;
&lt;p&gt;To run the benchmarks, I built a small CLI tool: &lt;a href=&#34;https://github.com/KonMam/compressbench&#34;&gt;compressbench&lt;/a&gt;. It’s publicly available and it currently supports gzip, snappy, lz4, and zstd (with levels) and outputs compression/decompression benchmarks for Parquet files. I’m planning to add support for custom codecs later, mostly so I can benchmark my own RLE, Huffman, and LZ77 implementations.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/posts/which-compression-algorithm-saves-most/image-1.png&#34; alt=&#34;Compression Ratio/Time/Throughput, Decompression Throughput&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;compression-ratio&#34;&gt;Compression Ratio&lt;/h3&gt;
&lt;p&gt;If you only care about the smallest file, zstd-19 and gzip come out ahead. But the margin over zstd-3 is tiny, and you pay for it heavily elsewhere. Snappy and LZ4 compress to about 1.12 - just enough to make it look like they tried. But if that’s all you have, 12% savings is still better than no savings.&lt;/p&gt;
&lt;p&gt;For most storage use cases, zstd-3 gets close enough to the “best” ratio without turning your CPU into a space heater.&lt;/p&gt;
&lt;h3 id=&#34;compression-speed&#34;&gt;Compression Speed&lt;/h3&gt;
&lt;p&gt;Snappy and LZ4 are fast. zstd-1, 3, and 9 kept up surprisingly well. gzip is predictably slow. zstd-19 made me question my life choices, I thought it might have frozen or got silently murdered by the OS. I’m not saying never use it, there are some use cases, but they’re likely few and far between.&lt;/p&gt;
&lt;h3 id=&#34;decompression-speed&#34;&gt;Decompression Speed&lt;/h3&gt;
&lt;p&gt;Snappy and LZ4 hit over 3.5GB/s. zstd held steady around 1GB/s across all levels. gzip stayed slow.&lt;/p&gt;
&lt;p&gt;If you need to read the same data multiple times Snappy and LZ4 are faster than gzip or zstd. But zstd isn’t slow enough to matter unless your volumes are huge.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;file-size-scaling&#34;&gt;File Size Scaling&lt;/h3&gt;
&lt;p&gt;Throughput went down as file size grew. Gzip was slow the whole time. zstd-19 was even slower and I didn&amp;rsquo;t run it for all file size, so it may have gone even more down.&lt;/p&gt;
&lt;p&gt;The others held up fairly well. Snappy stayed fastest, but none of them completely fell apart.&lt;/p&gt;
&lt;p&gt;Note: CPU was pinned at 100% during all runs. On a single-threaded, 16GB machine, there was probably some memory pressure too for the larger files. These results match what I’ve seen elsewhere but might be a bit exaggerated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/posts/which-compression-algorithm-saves-most/image-2.png&#34; alt=&#34;Compression Throughput vs File Size&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;storage-cost-s3&#34;&gt;Storage Cost (S3)&lt;/h3&gt;
&lt;p&gt;S3 Standard pricing in eu-central-1 is $0.0235/GB. At 500TB/month, codec choice can have a significant (based on your budged) impact on the cost. But if you&amp;rsquo;re only storing a few TB, this doesn’t matter much. At 100TB, you&amp;rsquo;re looking at maybe a few hundred bucks.&lt;/p&gt;
&lt;p&gt;Snappy/LZ4 would cost around $10.7K/month. zstd-3 lands near $9.7K for 500TBs. zstd-19 saves a bit more, but the compute cost and latency make it hard to justify. gzip is in the same ballpark, and we’ve already covered its performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/posts/which-compression-algorithm-saves-most/image-3.png&#34; alt=&#34;Projected Monthly S3 Cost by Codec at Scale&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-to-pick&#34;&gt;What to Pick?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;For streaming&lt;/strong&gt;
Snappy or LZ4. Fast compression and decompression. Compression ratio better than nothing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For batch ETL or periodic jobs&lt;/strong&gt;
zstd-1 or zstd-3. Good balance between speed and size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For archival&lt;/strong&gt;
zstd-9 if you care for small gains. zstd-19 if you’re archiving something you hope nobody ever reads again.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;After my initial post, I assumed the real-life impact of LZ4 and zstd would be more obvious. But it turns out you need quite a bit of scale to feel it. In the future, I won’t be so quick to dismiss Snappy as it has its place. But it’s not the only viable option there is.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d also like to benchmark compute cost in the future and see whether using zstd at scale is actually worth it for batch processes or if the additional compute time eats up your storage savings.&lt;/p&gt;
&lt;p&gt;Also keep in mind that your mileage might vary based on your data, compression is about finding patters and if there&amp;rsquo;s none, the result might be a larger file than you began with, so pick accordingly, maybe run a benchmark yourself.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thank you for reading.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Compression Algorithms You Probably Inherited: gzip, Snappy, LZ4, zstd</title>
      <link>http://localhost:1313/posts/compression-algorithms-you-probably-inherited/</link>
      <pubDate>Mon, 18 Aug 2025 21:45:14 +0300</pubDate>
      
      <guid>http://localhost:1313/posts/compression-algorithms-you-probably-inherited/</guid>
      <description>&lt;h2 id=&#34;you-might-be-using-the-wrong-compression-algorithm&#34;&gt;You Might Be Using The Wrong Compression Algorithm&lt;/h2&gt;
&lt;p&gt;If you work in data engineering, you’ve probably used &lt;strong&gt;gzip&lt;/strong&gt;, &lt;strong&gt;Snappy&lt;/strong&gt;, &lt;strong&gt;LZ4&lt;/strong&gt;, or &lt;strong&gt;Zstandard (zstd)&lt;/strong&gt;. More likely - you inherited them. Either the person who set these defaults is long gone, there’s never enough time to revisit the choice, or things work well enough and you’d rather not duck around and find out otherwise.&lt;/p&gt;
&lt;p&gt;Most engineers stick with the defaults. Changing them feels risky. And let’s be honest - many don’t really know what these algorithms do or why one was chosen in the first place.&lt;/p&gt;</description>
      <content>&lt;h2 id=&#34;you-might-be-using-the-wrong-compression-algorithm&#34;&gt;You Might Be Using The Wrong Compression Algorithm&lt;/h2&gt;
&lt;p&gt;If you work in data engineering, you’ve probably used &lt;strong&gt;gzip&lt;/strong&gt;, &lt;strong&gt;Snappy&lt;/strong&gt;, &lt;strong&gt;LZ4&lt;/strong&gt;, or &lt;strong&gt;Zstandard (zstd)&lt;/strong&gt;. More likely - you inherited them. Either the person who set these defaults is long gone, there’s never enough time to revisit the choice, or things work well enough and you’d rather not duck around and find out otherwise.&lt;/p&gt;
&lt;p&gt;Most engineers stick with the defaults. Changing them feels risky. And let’s be honest - many don’t really know what these algorithms do or why one was chosen in the first place.&lt;/p&gt;
&lt;p&gt;I’ve been that person myself: &lt;em&gt;&amp;ldquo;Oh, we’re using Snappy? OK.&amp;rdquo;&lt;/em&gt; Never thinking to ask why or what else we could use.&lt;/p&gt;
&lt;p&gt;This post explains the most common compression algorithms, what makes them different, and when you should actually use each.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;why-compression-choices-matter&#34;&gt;Why Compression Choices Matter&lt;/h2&gt;
&lt;p&gt;Compression decisions aren’t just about saving space. They directly impact:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Storage costs&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU utilization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In modern pipelines — Kafka, Parquet, column stores, data lakes - the wrong compression algorithm can degrade all of these.&lt;/p&gt;
&lt;p&gt;Two metrics matter most:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Compression ratio&lt;/strong&gt;: How much smaller the data gets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Throughput&lt;/strong&gt;: How quickly data can be compressed and decompressed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your workload - and whether you prioritize CPU, latency, or bandwidth - determines which trade-offs are acceptable.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;main-culprits&#34;&gt;Main Culprits&lt;/h2&gt;
&lt;h3 id=&#34;gzip&#34;&gt;gzip&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What it is&lt;/strong&gt;: Uses the DEFLATE algorithm (LZ77 + Huffman coding).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Good compression ratio. Compatibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Slow to compress. Moderate decompression speed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strength&lt;/strong&gt;: Ubiquitous. Supported everywhere.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weakness&lt;/strong&gt;: Outclassed in both speed and compression ratio by newer algorithms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;When to use&lt;/strong&gt;: Archival, compatibility with legacy tools. Otherwise, avoid.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;snappy&#34;&gt;Snappy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What it is&lt;/strong&gt;: Developed by Google. Based on LZ77 without entropy coding.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Maximize speed, not compression ratio.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Very fast compression and decompression.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strength&lt;/strong&gt;: Low CPU overhead. Stable. Production-proven at Google scale.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weakness&lt;/strong&gt;: Larger compressed size than other options.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;When to use&lt;/strong&gt;: Real-time, low-CPU systems where latency matters more than storage. Or if you&amp;rsquo;re stuck with it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lz4&#34;&gt;LZ4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What it is&lt;/strong&gt;: LZ77-based. Prioritizes speed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Fast compression and decompression with moderate compression ratio.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: &amp;gt; 500 MB/s compression. GB/s decompression.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strength&lt;/strong&gt;: Extremely fast. Low CPU usage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weakness&lt;/strong&gt;: Compression ratio lower than gzip or zstd.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;When to use&lt;/strong&gt;: High-throughput, low-latency systems. Datacenter transfers. OLAP engines (DuckDB, Cassandra).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;zstd-zstandard&#34;&gt;zstd (Zstandard)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What it is&lt;/strong&gt;: Developed by Facebook. Combines LZ77, Huffman coding, and FSE.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: High compression ratio with fast speed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Compression 500+ MB/s. Decompression ~1500+ MB/s.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strength&lt;/strong&gt;: Tunable. Balances speed and compression. Strong performance across data types.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weakness&lt;/strong&gt;: Slightly more CPU than LZ4/Snappy at default settings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;When to use&lt;/strong&gt;: General-purpose. Parquet files. Kafka. Data transfers. Usually the best all-around choice.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;strengths-and-weaknesses-at-a-glance&#34;&gt;Strengths and Weaknesses (At a Glance)&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Algorithm&lt;/th&gt;
          &lt;th&gt;Compression Ratio&lt;/th&gt;
          &lt;th&gt;Compression Speed&lt;/th&gt;
          &lt;th&gt;Decompression Speed&lt;/th&gt;
          &lt;th&gt;Best For&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;gzip&lt;/td&gt;
          &lt;td&gt;Moderate&lt;/td&gt;
          &lt;td&gt;Slow&lt;/td&gt;
          &lt;td&gt;Moderate&lt;/td&gt;
          &lt;td&gt;Archival, web content&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Snappy&lt;/td&gt;
          &lt;td&gt;Low&lt;/td&gt;
          &lt;td&gt;Very Fast&lt;/td&gt;
          &lt;td&gt;Very Fast&lt;/td&gt;
          &lt;td&gt;Real-time, low-CPU systems&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;LZ4&lt;/td&gt;
          &lt;td&gt;Moderate&lt;/td&gt;
          &lt;td&gt;Extremely Fast&lt;/td&gt;
          &lt;td&gt;Extremely Fast&lt;/td&gt;
          &lt;td&gt;High-throughput, low-latency systems&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;zstd&lt;/td&gt;
          &lt;td&gt;High&lt;/td&gt;
          &lt;td&gt;Fast&lt;/td&gt;
          &lt;td&gt;Fast&lt;/td&gt;
          &lt;td&gt;General-purpose, Parquet, Kafka, data transfers&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;real-world-scenarios-when-to-use-what&#34;&gt;Real-World Scenarios: When to Use What&lt;/h2&gt;
&lt;h3 id=&#34;high-throughput-streaming-kafka&#34;&gt;High-throughput streaming (Kafka)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;: zstd or LZ4&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt;: zstd gives better compression with good speed. LZ4 if latency is critical and CPU is limited. Snappy is acceptable if inherited, but usually not optimal anymore.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;long-term-storage-parquet-s3&#34;&gt;Long-term storage (Parquet, S3)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;: zstd&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt;: Best compression ratio reduces storage cost and IO. Slight CPU trade-off is acceptable.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;low-latency-querying-duckdb-cassandra&#34;&gt;Low-latency querying (DuckDB, Cassandra)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;: LZ4&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt;: Prioritize decompression speed for fast queries. LZ4 is the common choice in OLAP engines.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cpumemory-constrained-environments&#34;&gt;CPU/memory constrained environments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;: Snappy or LZ4&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt;: Low CPU overhead is more important than compression ratio. zstd can still be used at low compression levels if needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fast-network-low-compression-benefit-datacenter-file-transfer&#34;&gt;Fast network, low compression benefit (datacenter file transfer)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;: LZ4&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt;: Minimal compression overhead. On fast networks, speed beats smaller file sizes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;slow-network-or-internet-transfers&#34;&gt;Slow network or internet transfers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;: zstd&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt;: Better compression reduces transfer time despite slightly higher CPU cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-to-remember&#34;&gt;What to Remember&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;No algorithm is best for every workload.&lt;/li&gt;
&lt;li&gt;zstd has become the Swiss Army knife of compression. Unless you have a good reason not to, it’s a smart pick.&lt;/li&gt;
&lt;li&gt;LZ4 is unbeatable when speed matters more than compression.&lt;/li&gt;
&lt;li&gt;Snappy is still acceptable in latency-sensitive, CPU-constrained setups but is generally being replaced.&lt;/li&gt;
&lt;li&gt;gzip remains for legacy systems or when maximum compatibility is required.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;whats-underneath-the-hood&#34;&gt;What&amp;rsquo;s Underneath The Hood&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LZ77&lt;/strong&gt; - Replaces repeated sequences of data with references to earlier copies in the stream (sliding window). &lt;a href=&#34;https://en.wikipedia.org/wiki/LZ77_and_LZ78&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Huffman Coding&lt;/strong&gt; - A method of assigning shorter codes to more frequent data patterns to save space. &lt;a href=&#34;https://en.wikipedia.org/wiki/Huffman_coding&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FSE (Finite State Entropy)&lt;/strong&gt; - An advanced entropy coding method that efficiently compresses sequences by balancing speed and compression ratio. &lt;a href=&#34;https://facebook.github.io/zstd/&#34;&gt;Facebook’s zstd Manual&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;
Most compression algorithms combine finding patterns (LZ77) with efficient encoding (Huffman, FSE) to shrink data without losing information.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;closing-thoughts&#34;&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;Compression choices tend to stick around. There’s rarely time to revisit legacy pipelines, and if something works, it’s easy to assume it’s good enough. But if you can make the time, you’re now better equipped to review your defaults (I know I am.) - and see if a different choice might better fit your needs.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thank you for reading.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>DuckDB: When You Don’t Need Spark (But Still Need SQL)</title>
      <link>http://localhost:1313/posts/duckdb-when-you-dont-need-spark/</link>
      <pubDate>Mon, 18 Aug 2025 21:43:23 +0300</pubDate>
      
      <guid>http://localhost:1313/posts/duckdb-when-you-dont-need-spark/</guid>
      <description>&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Too often, data engineering tasks that should be simple end up requiring heavyweight tools. Something breaks, or I need to explore a new dataset, and suddenly I’m firing up Spark or connecting to a cloud warehouse - even though the data easily fits on my laptop. That adds extra steps, slows things down, and costs more than it should. I wanted something simpler for local analytics that could still handle serious queries.&lt;/p&gt;</description>
      <content>&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Too often, data engineering tasks that should be simple end up requiring heavyweight tools. Something breaks, or I need to explore a new dataset, and suddenly I’m firing up Spark or connecting to a cloud warehouse - even though the data easily fits on my laptop. That adds extra steps, slows things down, and costs more than it should. I wanted something simpler for local analytics that could still handle serious queries.&lt;/p&gt;
&lt;h2 id=&#34;what-is-duckdb&#34;&gt;What Is DuckDB?&lt;/h2&gt;
&lt;p&gt;DuckDB is an open-source, in-process SQL OLAP database designed for analytics.&lt;/p&gt;
&lt;p&gt;It runs embedded inside applications, similar to SQLite, but optimized for analytical queries like joins, aggregations, and large scans.&lt;/p&gt;
&lt;p&gt;In short, it goes fast without adding the complexity of distributed systems.&lt;/p&gt;
&lt;h2 id=&#34;how-duckdb-achieves-high-performance&#34;&gt;How DuckDB Achieves High Performance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Columnar Storage:&lt;/strong&gt;
Data is stored by columns, not rows. This lets queries scan only the data they need, cutting down IO.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vectorized Execution:&lt;/strong&gt;
Processes data in batches (about 1000 rows at a time) to leverage CPU caching and SIMD instructions, reducing processing overhead.&lt;/p&gt;
&lt;p&gt;These two design choices allow DuckDB to handle complex analytical queries efficiently on a single machine.&lt;/p&gt;
&lt;h2 id=&#34;handling-large-datasets&#34;&gt;Handling Large Datasets&lt;/h2&gt;
&lt;p&gt;DuckDB dynamically manages memory and disk usage based on workload size:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;In-Memory Mode:&lt;/strong&gt; Keeps everything in RAM if possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Out-of-Core Mode:&lt;/strong&gt; Spills to disk if data exceeds memory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid Execution:&lt;/strong&gt; Switches between modes automatically based on workload.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Persistent Storage:&lt;/strong&gt; Can save results in &lt;code&gt;.duckdb&lt;/code&gt; files for reuse.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No manual configuration. No crashing on out-of-memory errors (Hi Pandas!).&lt;/p&gt;
&lt;h2 id=&#34;extensibility--concurrency&#34;&gt;Extensibility &amp;amp; Concurrency&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Single-writer, multiple-reader concurrency (MVCC).&lt;/li&gt;
&lt;li&gt;Growing ecosystem of extensions: Parquet, CSV, S3, HTTP endpoints, geospatial analytics.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;trade-offs-duckdb-vs-specialized-engines&#34;&gt;Trade-Offs: DuckDB vs Specialized Engines&lt;/h2&gt;
&lt;p&gt;DuckDB is flexible and fast, but:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SQL Parsing Overhead:&lt;/strong&gt; Engines like Polars can be faster for simple dataframe operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;General Purpose Design:&lt;/strong&gt; Flexibility trades off some raw speed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That said, for most data engineering tasks, the trade-off is worth it.&lt;/p&gt;
&lt;h2 id=&#34;where-duckdb-shines&#34;&gt;Where DuckDB Shines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Local dataset exploration (when Pandas hits limits).&lt;/li&gt;
&lt;li&gt;CI and pipeline testing without Spark.&lt;/li&gt;
&lt;li&gt;Batch transformations on Parquet, CSV, and other formats.&lt;/li&gt;
&lt;li&gt;Lightweight production workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limits-to-keep-in-mind&#34;&gt;Limits to Keep in Mind&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Single-machine only - limited by your hardware.&lt;/li&gt;
&lt;li&gt;Not built for transactional workloads.&lt;/li&gt;
&lt;li&gt;SQL pipelines can get messy if not managed well.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reflection-why-this-matters&#34;&gt;Reflection: Why This Matters&lt;/h2&gt;
&lt;p&gt;DuckDB helps bridge the gap between dataset size and engineering overhead.It’s not about replacing big tools, but avoiding them when you don’t need them.&lt;/p&gt;
&lt;p&gt;For tasks that outgrow Pandas or require complex queries, it’s a practical alternative to heavier tools.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
