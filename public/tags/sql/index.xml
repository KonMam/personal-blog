<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sql on mamonas.dev</title>
    <link>http://localhost:1313/tags/sql/</link>
    <description>Recent content in Sql on mamonas.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Aug 2025 21:43:23 +0300</lastBuildDate><atom:link href="http://localhost:1313/tags/sql/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DuckDB: When You Don’t Need Spark (But Still Need SQL)</title>
      <link>http://localhost:1313/posts/duckdb-when-you-dont-need-spark/</link>
      <pubDate>Mon, 18 Aug 2025 21:43:23 +0300</pubDate>
      
      <guid>http://localhost:1313/posts/duckdb-when-you-dont-need-spark/</guid>
      <description>&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Too often, data engineering tasks that should be simple end up requiring heavyweight tools. Something breaks, or I need to explore a new dataset, and suddenly I’m firing up Spark or connecting to a cloud warehouse - even though the data easily fits on my laptop. That adds extra steps, slows things down, and costs more than it should. I wanted something simpler for local analytics that could still handle serious queries.&lt;/p&gt;</description>
      <content>&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Too often, data engineering tasks that should be simple end up requiring heavyweight tools. Something breaks, or I need to explore a new dataset, and suddenly I’m firing up Spark or connecting to a cloud warehouse - even though the data easily fits on my laptop. That adds extra steps, slows things down, and costs more than it should. I wanted something simpler for local analytics that could still handle serious queries.&lt;/p&gt;
&lt;h2 id=&#34;what-is-duckdb&#34;&gt;What Is DuckDB?&lt;/h2&gt;
&lt;p&gt;DuckDB is an open-source, in-process SQL OLAP database designed for analytics.&lt;/p&gt;
&lt;p&gt;It runs embedded inside applications, similar to SQLite, but optimized for analytical queries like joins, aggregations, and large scans.&lt;/p&gt;
&lt;p&gt;In short, it goes fast without adding the complexity of distributed systems.&lt;/p&gt;
&lt;h2 id=&#34;how-duckdb-achieves-high-performance&#34;&gt;How DuckDB Achieves High Performance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Columnar Storage:&lt;/strong&gt;
Data is stored by columns, not rows. This lets queries scan only the data they need, cutting down IO.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vectorized Execution:&lt;/strong&gt;
Processes data in batches (about 1000 rows at a time) to leverage CPU caching and SIMD instructions, reducing processing overhead.&lt;/p&gt;
&lt;p&gt;These two design choices allow DuckDB to handle complex analytical queries efficiently on a single machine.&lt;/p&gt;
&lt;h2 id=&#34;handling-large-datasets&#34;&gt;Handling Large Datasets&lt;/h2&gt;
&lt;p&gt;DuckDB dynamically manages memory and disk usage based on workload size:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;In-Memory Mode:&lt;/strong&gt; Keeps everything in RAM if possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Out-of-Core Mode:&lt;/strong&gt; Spills to disk if data exceeds memory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid Execution:&lt;/strong&gt; Switches between modes automatically based on workload.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Persistent Storage:&lt;/strong&gt; Can save results in &lt;code&gt;.duckdb&lt;/code&gt; files for reuse.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No manual configuration. No crashing on out-of-memory errors (Hi Pandas!).&lt;/p&gt;
&lt;h2 id=&#34;extensibility--concurrency&#34;&gt;Extensibility &amp;amp; Concurrency&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Single-writer, multiple-reader concurrency (MVCC).&lt;/li&gt;
&lt;li&gt;Growing ecosystem of extensions: Parquet, CSV, S3, HTTP endpoints, geospatial analytics.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;trade-offs-duckdb-vs-specialized-engines&#34;&gt;Trade-Offs: DuckDB vs Specialized Engines&lt;/h2&gt;
&lt;p&gt;DuckDB is flexible and fast, but:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SQL Parsing Overhead:&lt;/strong&gt; Engines like Polars can be faster for simple dataframe operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;General Purpose Design:&lt;/strong&gt; Flexibility trades off some raw speed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That said, for most data engineering tasks, the trade-off is worth it.&lt;/p&gt;
&lt;h2 id=&#34;where-duckdb-shines&#34;&gt;Where DuckDB Shines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Local dataset exploration (when Pandas hits limits).&lt;/li&gt;
&lt;li&gt;CI and pipeline testing without Spark.&lt;/li&gt;
&lt;li&gt;Batch transformations on Parquet, CSV, and other formats.&lt;/li&gt;
&lt;li&gt;Lightweight production workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limits-to-keep-in-mind&#34;&gt;Limits to Keep in Mind&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Single-machine only - limited by your hardware.&lt;/li&gt;
&lt;li&gt;Not built for transactional workloads.&lt;/li&gt;
&lt;li&gt;SQL pipelines can get messy if not managed well.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reflection-why-this-matters&#34;&gt;Reflection: Why This Matters&lt;/h2&gt;
&lt;p&gt;DuckDB helps bridge the gap between dataset size and engineering overhead.It’s not about replacing big tools, but avoiding them when you don’t need them.&lt;/p&gt;
&lt;p&gt;For tasks that outgrow Pandas or require complex queries, it’s a practical alternative to heavier tools.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
